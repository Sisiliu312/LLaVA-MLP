import os
import numpy as np
import torch
import torch.nn as nn
import re
import torch.nn.functional as F
from datetime import datetime

class IdentityMap(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x, *args, **kwargs):
        return x

    @property
    def config(self):
        return {"mm_projector_type": 'identity'}


class SimpleResBlock(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.pre_norm = nn.LayerNorm(channels)

        self.proj = nn.Sequential(
            nn.Linear(channels, channels),
            nn.GELU(),
            nn.Linear(channels, channels)
        )
    def forward(self, x):
        x = self.pre_norm(x)
        return x + self.proj(x)

class AttentionWeightSaver:
    def __init__(self, save_dir='attention_weights', format='pt'):
        """
        ÂàùÂßãÂåñ‰øùÂ≠òÂô®
        :param save_dir: ‰øùÂ≠òÁõÆÂΩïË∑ØÂæÑ
        :param format: ‰øùÂ≠òÊ†ºÂºè ('pt' for PyTorch, 'npy' for NumPy)
        """
        self.save_dir = save_dir
        self.format = format
        self.counter = 0
        self._create_save_dir()
        
    def _create_save_dir(self):
        """ÂàõÂª∫‰øùÂ≠òÁõÆÂΩï"""
        if not os.path.exists(self.save_dir):
            os.makedirs(self.save_dir)
            
    def _generate_filename(self):
        """ÁîüÊàêÊåâÂ∫èÂè∑ÈÄíÂ¢ûÁöÑÊñá‰ª∂Âêç"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"attn_{self.counter:04d}_{timestamp}"
        self.counter += 1
        return os.path.join(self.save_dir, f"{filename}.{self.format}")
    
    def save(self, attn_weights, metadata=None):
        """
        ‰øùÂ≠òattention weights
        :param attn_weights: Ë¶Å‰øùÂ≠òÁöÑattention weightsÂº†Èáè
        :param metadata: ÂèØÈÄâÁöÑÂÖÉÊï∞ÊçÆÂ≠óÂÖ∏
        """
        filename = self._generate_filename()
        
        if self.format == 'pt':
            # ‰øùÂ≠ò‰∏∫PyTorchÊñá‰ª∂
            save_dict = {'attn_weights': attn_weights}
            if metadata:
                save_dict['metadata'] = metadata
            torch.save(save_dict, filename)
        elif self.format == 'npy':
            # ‰øùÂ≠ò‰∏∫NumPyÊñá‰ª∂
            if isinstance(attn_weights, torch.Tensor):
                attn_weights = attn_weights.cpu().numpy()
            if metadata:
                np.savez(filename, attn_weights=attn_weights, **metadata)
            else:
                np.save(filename, attn_weights)
        else:
            raise ValueError(f"Unsupported format: {self.format}")
            
        print(f"Saved attention weights to: {filename}")
        return filename
    
saver = AttentionWeightSaver(save_dir='/home/data/shika/LLaVA/playground/data/eval/textvqa', format='pt')

class CrossAttention(nn.Module):
    def __init__(self, text_dim, feature_dim):
        super(CrossAttention, self).__init__()
        # ÂàùÂßãÂåñÁ∫øÊÄßÂèòÊç¢Â±Ç
        self.text_dim = text_dim
        self.feature_dim = feature_dim
        self.W_q = nn.Linear(text_dim, feature_dim)
        self.W_k = nn.Linear(feature_dim, feature_dim)
        
        # ÂàùÂßãÂåñÂèÇÊï∞
        self._reset_parameters()
        print('ÂàùÂßãÂåñÊó∂ self.W_q :', self.W_q.weight.norm().item())
        print('ÂàùÂßãÂåñÊó∂ self.W_q :', self.W_q.weight.shape)
        print('ÂàùÂßãÂåñÊó∂ self.W_q.bias :', self.W_k.bias.shape)

    def _reset_parameters(self):
        nn.init.xavier_uniform_(self.W_q.weight)
        nn.init.xavier_uniform_(self.W_k.weight)
        nn.init.constant_(self.W_q.bias, 0.0)
        nn.init.constant_(self.W_k.bias, 0.0)

    def forward(self, text, features):
        """
        text: [batch, text_seq_len, text_dim] ÊñáÊú¨ÁâπÂæÅ
        features: [batch, num_patches, feature_dim] ÂõæÂÉèÁâπÂæÅ
        ËøîÂõû: [batch, num_patches] Ê≥®ÊÑèÂäõÊùÉÈáç
        """
        
        print('fordwardÊó∂ self.W_q mean:', self.W_q.weight.norm().item())
        print('fordwardÊó∂ self.W_q :', self.W_q.weight.shape)
        print('fordwardÊó∂ self.W_q.bias :', self.W_k.bias.shape)

        # 1. Á∫øÊÄßÂèòÊç¢
        Q = self.W_q(text)  # [batch, text_seq_len, feature_dim]
        K = self.W_k(features)  # [batch, num_patches, feature_dim]
        # print(f"K min: {K.min()}, max: {K.max()}")

        # Q = Q.clamp(min=-50, max=50)
        # K = K.clamp(min=-50, max=50)
        
        # # 2. Â∫îÁî®Â±ÇÂΩí‰∏ÄÂåñ
        # Q = self.q_norm(Q)
        # K = self.k_norm(K)
        # print('Q mean:', Q.mean(), 'std:', Q.std())
        # print('K mean:', K.mean(), 'std:', K.std())
        # print(f"K min: {K.min()}, max: {K.max()}")
        
        # 3. ËÆ°ÁÆóÊ≥®ÊÑèÂäõÂàÜÊï∞
        attn_scores = torch.matmul(Q, K.transpose(1, 2))  # [batch, text_seq_len, num_patches]

        # ‚úÖ Ê≥®ÊÑèÔºöËøôÈáåÈªòËÆ§‰∏çÂÜçËøõË°åÁº©Êîæ
        attn_scores = attn_scores / (K.size(-1) ** 0.5)
        # print("attn_scores std:", attn_scores.std().item())
        # print("attn_scores min:", attn_scores.min().item())
        # print("attn_scores max:", attn_scores.max().item())

        # 4. Â∫îÁî® softmax Ëé∑ÂèñÊ≥®ÊÑèÂäõÊùÉÈáç
        attn_weights = torch.softmax(attn_scores, dim=-1)
        # print("ÊùÉÈáç‰∏∫:",attn_weights.mean())
        
        # saver.save(attn_weights)

        attended = torch.matmul(attn_weights, features)  # [B, T, D]

        return attended

class LayerSelectionRouter(nn.Module):
    """
    Router that selects top-5 layers from 24 vision tower layers.
    Initialized to uniformly select layers [1, 6, 11, 16, 21] (0-indexed: [0, 5, 10, 15, 20])
    """
    def __init__(self, dim, num_layers, top_router):
        super(LayerSelectionRouter, self).__init__()
        self.num_layers = num_layers
        self.top_router = top_router
        self.dim = dim

        # print(f"Initializing LayerSelectionRouter with dim={self.dim}, num_layers={self.num_layers}, top_router={self.top_router}")
        
        # Router network with SiLU gating (matching diagram: W1, W2, W3)
        self.w1 = nn.Linear(dim, dim)
        self.w2 = nn.Linear(dim, dim)
        self.w3 = nn.Linear(dim, num_layers)
        
        # Initialize to favor uniform selection of [0, 5, 10, 15, 20]
        self._reset_parameters()
        print('ÂàùÂßãÂåñÊó∂ self.w1 mean:', self.w1.weight.norm().item())
        print('ÂàùÂßãÂåñÊó∂ self.w1 shape:', self.w1.weight.shape)
        print("ÂàùÂßãÂåñÊó∂ w3.bias (Ââç6 + Âêé1):", self.w3.bias.tolist()[:6] + ["..."] + [self.w3.bias.tolist()[-1]])
        
    def _reset_parameters(self):
        """Initialize router to uniformly select layers [1, 6, 11, 16, 21] (0-indexed)"""
        
        checkpoint = torch.load('/hy-tmp/checkpoints/llava-v1.5-7b-pretrain/mm_projector.bin')
        print("‰øùÂ≠òÁöÑÂèÇÊï∞:")
        for key in checkpoint.keys():
            print(f"  {key}: {checkpoint[key].shape}")


        nn.init.xavier_uniform_(self.w1.weight)
        nn.init.xavier_uniform_(self.w2.weight)
        nn.init.xavier_uniform_(self.w3.weight)
        nn.init.constant_(self.w1.bias, 0.0)
        nn.init.constant_(self.w2.bias, 0.0)
        nn.init.constant_(self.w3.bias, -0.1)
        
        with torch.no_grad():
            uniform_indices = [3, 8, 13, 18, 23]
            for idx in uniform_indices:
                self.w3.bias[idx] = 0.1
                    
    
    def forward(self, text_features):
        """
        Args:
            text_features: Text token features [batch_size, text_len, dim]
        
        Returns:
            layer_weights: Softmax weights for all layers [batch_size, num_layers]
            selected_indices: Indices of top-k selected layers [batch_size, top_router]
        """
        # [batch_size, text_len, dim] -> [batch_size, dim]
        pooled = text_features.mean(dim=1)
        # self.attention_pool = nn.Sequential(
        #     nn.Linear(dim, 1),
        #     nn.Softmax(dim=1)
        # )
        
        print('fordwardÊó∂ self.w1 mean:', self.w1.weight.norm().item())
        print('fordwardÊó∂ self.w1 shape:', self.w1.weight.shape)
        h1 = F.silu(self.w1(pooled))
        
        h2 = F.silu(self.w2(pooled))
        
        gated = h1 * h2
        
        print("forwardÊó∂ w3.bias (Ââç6 + Âêé1):", self.w3.bias.tolist()[:6] + ["..."] + [self.w3.bias.tolist()[-1]])
        logits = self.w3(gated)
        
        layer_probs = F.softmax(logits, dim=-1)

        top_weights, top_indices = torch.topk(layer_probs, self.top_router, dim=-1)
        top_weights = top_weights / top_weights.sum(dim=-1, keepdim=True)
        
        return top_indices, top_weights, layer_probs

def build_vision_projector(config, delay_load=False, **kwargs):
    projector_type = getattr(config, 'mm_projector_type', 'linear')

    if projector_type == 'linear':
        return nn.Linear(config.mm_hidden_size, config.hidden_size)

    mlp_gelu_match = re.match(r'^mlp(\d+)x_gelu$', projector_type)
    if mlp_gelu_match:
        mlp_depth = int(mlp_gelu_match.group(1))
        modules = [nn.Linear(config.mm_hidden_size, config.hidden_size)]
        for _ in range(1, mlp_depth):
            modules.append(nn.GELU())
            modules.append(nn.Linear(config.hidden_size, config.hidden_size))
        return nn.Sequential(*modules)

    if projector_type == 'identity':
        return IdentityMap()

    raise ValueError(f'Unknown projector type: {projector_type}')

def build_cross_attn(config):
    text_dim = config.text_dim if hasattr(config, 'text_dim') else 4096
    feature_dim = config.feature_dim if hasattr(config, 'feature_dim') else 4096
    
    return CrossAttention(text_dim=text_dim, feature_dim=feature_dim)

def build_layer_router(config):
    # print("=" * 60)
    # print("üîç build_layer_router Ë∞ÉËØï:")
    # print(f"  hasattr(config, 'dim'): {hasattr(config, 'dim')}")
    # print(f"  hasattr(config, 'num_layers'): {hasattr(config, 'num_layers')}")
    # print(f"  hasattr(config, 'top_router'): {hasattr(config, 'top_router')}")
    
    dim = config.dim if hasattr(config, 'dim') else 4096
    num_layers = config.num_layers if hasattr(config, 'num_layers') else 24
    top_router = config.top_router if hasattr(config, 'top_router') else 5
    
    print(f"  ÊúÄÁªà: dim={dim}, num_layers={num_layers}, top_router={top_router}")
    print(f"  Á±ªÂûã: dim={type(dim)}, num_layers={type(num_layers)}, top_router={type(top_router)}")
    print("=" * 60)
    
    return LayerSelectionRouter(dim=dim, num_layers=num_layers, top_router=top_router)